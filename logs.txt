
==> Audit <==
|------------|--------------------------------|----------|--------------|---------|-----------------------|-----------------------|
|  Command   |              Args              | Profile  |     User     | Version |      Start Time       |       End Time        |
|------------|--------------------------------|----------|--------------|---------|-----------------------|-----------------------|
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:03 +0545 | 20 May 24 12:05 +0545 |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:32 +0545 | 20 May 24 12:32 +0545 |
| dashboard  |                                | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:35 +0545 |                       |
| service    | hello-node                     | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:42 +0545 | 20 May 24 12:43 +0545 |
| config     | set driver docker              | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:44 +0545 | 20 May 24 12:44 +0545 |
| service    | hello-node                     | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:44 +0545 | 20 May 24 12:44 +0545 |
| service    | --all                          | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:45 +0545 | 20 May 24 12:45 +0545 |
| dashboard  |                                | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:45 +0545 |                       |
| addons     | list                           | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:46 +0545 | 20 May 24 12:46 +0545 |
| service    | --all                          | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:47 +0545 | 20 May 24 12:47 +0545 |
| stop       |                                | minikube | KISEE\LEGION | v1.33.1 | 20 May 24 12:48 +0545 | 20 May 24 12:49 +0545 |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 14:02 +0545 |                       |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 14:04 +0545 | 23 May 24 14:05 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 16:48 +0545 | 23 May 24 16:48 +0545 |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 16:56 +0545 | 23 May 24 16:57 +0545 |
| stop       |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 17:21 +0545 | 23 May 24 17:21 +0545 |
| delete     |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 17:21 +0545 | 23 May 24 17:21 +0545 |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 17:22 +0545 | 23 May 24 17:22 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 17:33 +0545 | 23 May 24 17:33 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 23 May 24 17:35 +0545 | 23 May 24 17:35 +0545 |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 07:30 +0545 | 24 May 24 07:30 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 07:30 +0545 | 24 May 24 07:31 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 07:31 +0545 | 24 May 24 07:31 +0545 |
| docker-env | minikube docker-env --shell    | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 07:31 +0545 | 24 May 24 07:31 +0545 |
|            | cmd                            |          |              |         |                       |                       |
| docker-env | minikube docker-env --shell    | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 07:35 +0545 | 24 May 24 07:35 +0545 |
|            | cmd                            |          |              |         |                       |                       |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 07:35 +0545 | 24 May 24 07:35 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 07:40 +0545 | 24 May 24 07:40 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:27 +0545 |                       |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:27 +0545 | 24 May 24 16:28 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:28 +0545 | 24 May 24 16:28 +0545 |
| config     | set driver docker              | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:29 +0545 | 24 May 24 16:29 +0545 |
| config     | set driver docker              | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:30 +0545 | 24 May 24 16:30 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:30 +0545 | 24 May 24 16:30 +0545 |
| start      |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:30 +0545 | 24 May 24 16:30 +0545 |
| start      | --driver=docker                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:31 +0545 | 24 May 24 16:31 +0545 |
| docker-env |                                | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:32 +0545 | 24 May 24 16:32 +0545 |
| docker-env | minikube docker-env --shell    | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:32 +0545 | 24 May 24 16:32 +0545 |
|            | cmd                            |          |              |         |                       |                       |
| service    | mlops-minikube --url           | minikube | KISEE\LEGION | v1.33.1 | 24 May 24 16:43 +0545 |                       |
|------------|--------------------------------|----------|--------------|---------|-----------------------|-----------------------|


==> Last Start <==
Log file created at: 2024/05/24 16:31:04
Running on machine: KISEE
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0524 16:31:04.442830    3448 out.go:291] Setting OutFile to fd 100 ...
I0524 16:31:04.443836    3448 out.go:338] TERM=,COLORTERM=, which probably does not support color
I0524 16:31:04.443836    3448 out.go:304] Setting ErrFile to fd 104...
I0524 16:31:04.443836    3448 out.go:338] TERM=,COLORTERM=, which probably does not support color
I0524 16:31:04.465715    3448 out.go:298] Setting JSON to false
I0524 16:31:04.472510    3448 start.go:129] hostinfo: {"hostname":"KISEE","uptime":111452,"bootTime":1716436112,"procs":384,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3593 Build 22631.3593","kernelVersion":"10.0.22631.3593 Build 22631.3593","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"2670402b-271f-4836-b1f3-f24a80e692a8"}
W0524 16:31:04.472510    3448 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0524 16:31:04.473551    3448 out.go:177] * minikube v1.33.1 on Microsoft Windows 11 Home 10.0.22631.3593 Build 22631.3593
I0524 16:31:04.474064    3448 notify.go:220] Checking for updates...
I0524 16:31:04.474592    3448 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0524 16:31:04.475107    3448 driver.go:392] Setting default libvirt URI to qemu:///system
I0524 16:31:04.616203    3448 docker.go:122] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I0524 16:31:04.624062    3448 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0524 16:31:04.845213    3448 info.go:266] docker info: {ID:97503f82-a117-4790-9396-b088ccd78fe5 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:72 OomKillDisable:true NGoroutines:91 SystemTime:2024-05-24 10:46:04.81389339 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:7195762688 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0524 16:31:04.846269    3448 out.go:177] * Using the docker driver based on existing profile
I0524 16:31:04.846796    3448 start.go:297] selected driver: docker
I0524 16:31:04.847320    3448 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LEGION:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0524 16:31:04.847320    3448 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0524 16:31:04.862023    3448 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0524 16:31:05.089273    3448 info.go:266] docker info: {ID:97503f82-a117-4790-9396-b088ccd78fe5 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:72 OomKillDisable:true NGoroutines:91 SystemTime:2024-05-24 10:46:05.059341018 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:7195762688 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I0524 16:31:05.116249    3448 cni.go:84] Creating CNI manager for ""
I0524 16:31:05.116249    3448 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0524 16:31:05.116249    3448 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LEGION:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0524 16:31:05.116828    3448 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0524 16:31:05.117333    3448 cache.go:121] Beginning downloading kic base image for docker with docker
I0524 16:31:05.118382    3448 out.go:177] * Pulling base image v0.0.44 ...
I0524 16:31:05.118910    3448 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0524 16:31:05.118910    3448 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0524 16:31:05.118910    3448 preload.go:147] Found local preload: C:\Users\LEGION\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0524 16:31:05.118910    3448 cache.go:56] Caching tarball of preloaded images
I0524 16:31:05.119436    3448 preload.go:173] Found C:\Users\LEGION\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0524 16:31:05.119436    3448 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0524 16:31:05.119436    3448 profile.go:143] Saving config to C:\Users\LEGION\.minikube\profiles\minikube\config.json ...
I0524 16:31:05.242009    3448 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0524 16:31:05.242009    3448 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0524 16:31:05.242009    3448 cache.go:194] Successfully downloaded all kic artifacts
I0524 16:31:05.242534    3448 start.go:360] acquireMachinesLock for minikube: {Name:mk9c15945663da0d955425990001aca5380421c3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0524 16:31:05.242534    3448 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0524 16:31:05.242534    3448 start.go:96] Skipping create...Using existing machine configuration
I0524 16:31:05.242534    3448 fix.go:54] fixHost starting:
I0524 16:31:05.266794    3448 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 16:31:05.379153    3448 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0524 16:31:05.379153    3448 fix.go:138] unexpected machine state, will restart: <nil>
I0524 16:31:05.380184    3448 out.go:177] * Updating the running docker "minikube" container ...
I0524 16:31:05.380717    3448 machine.go:94] provisionDockerMachine start ...
I0524 16:31:05.394375    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:05.535315    3448 main.go:141] libmachine: Using SSH client type: native
I0524 16:31:05.535315    3448 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa3c0] 0x4acfa0 <nil>  [] 0s} 127.0.0.1 49443 <nil> <nil>}
I0524 16:31:05.535315    3448 main.go:141] libmachine: About to run SSH command:
hostname
I0524 16:31:05.666302    3448 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0524 16:31:05.666302    3448 ubuntu.go:169] provisioning hostname "minikube"
I0524 16:31:05.674804    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:05.795070    3448 main.go:141] libmachine: Using SSH client type: native
I0524 16:31:05.795606    3448 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa3c0] 0x4acfa0 <nil>  [] 0s} 127.0.0.1 49443 <nil> <nil>}
I0524 16:31:05.795606    3448 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0524 16:31:05.936445    3448 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0524 16:31:05.944326    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:06.054159    3448 main.go:141] libmachine: Using SSH client type: native
I0524 16:31:06.054670    3448 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa3c0] 0x4acfa0 <nil>  [] 0s} 127.0.0.1 49443 <nil> <nil>}
I0524 16:31:06.054670    3448 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts;
			fi
		fi
I0524 16:31:06.176372    3448 main.go:141] libmachine: SSH cmd err, output: <nil>:
I0524 16:31:06.176372    3448 ubuntu.go:175] set auth options {CertDir:C:\Users\LEGION\.minikube CaCertPath:C:\Users\LEGION\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\LEGION\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\LEGION\.minikube\machines\server.pem ServerKeyPath:C:\Users\LEGION\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\LEGION\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\LEGION\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\LEGION\.minikube}
I0524 16:31:06.176372    3448 ubuntu.go:177] setting up certificates
I0524 16:31:06.176372    3448 provision.go:84] configureAuth start
I0524 16:31:06.184207    3448 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0524 16:31:06.297037    3448 provision.go:143] copyHostCerts
I0524 16:31:06.297554    3448 exec_runner.go:144] found C:\Users\LEGION\.minikube/ca.pem, removing ...
I0524 16:31:06.297554    3448 exec_runner.go:203] rm: C:\Users\LEGION\.minikube\ca.pem
I0524 16:31:06.298088    3448 exec_runner.go:151] cp: C:\Users\LEGION\.minikube\certs\ca.pem --> C:\Users\LEGION\.minikube/ca.pem (1078 bytes)
I0524 16:31:06.298624    3448 exec_runner.go:144] found C:\Users\LEGION\.minikube/cert.pem, removing ...
I0524 16:31:06.298624    3448 exec_runner.go:203] rm: C:\Users\LEGION\.minikube\cert.pem
I0524 16:31:06.298624    3448 exec_runner.go:151] cp: C:\Users\LEGION\.minikube\certs\cert.pem --> C:\Users\LEGION\.minikube/cert.pem (1119 bytes)
I0524 16:31:06.299696    3448 exec_runner.go:144] found C:\Users\LEGION\.minikube/key.pem, removing ...
I0524 16:31:06.299696    3448 exec_runner.go:203] rm: C:\Users\LEGION\.minikube\key.pem
I0524 16:31:06.299696    3448 exec_runner.go:151] cp: C:\Users\LEGION\.minikube\certs\key.pem --> C:\Users\LEGION\.minikube/key.pem (1675 bytes)
I0524 16:31:06.300228    3448 provision.go:117] generating server cert: C:\Users\LEGION\.minikube\machines\server.pem ca-key=C:\Users\LEGION\.minikube\certs\ca.pem private-key=C:\Users\LEGION\.minikube\certs\ca-key.pem org=LEGION.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0524 16:31:06.431038    3448 provision.go:177] copyRemoteCerts
I0524 16:31:06.442139    3448 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0524 16:31:06.450720    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:06.559858    3448 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49443 SSHKeyPath:C:\Users\LEGION\.minikube\machines\minikube\id_rsa Username:docker}
I0524 16:31:06.660522    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0524 16:31:06.681515    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I0524 16:31:06.704087    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0524 16:31:06.725674    3448 provision.go:87] duration metric: took 549.3015ms to configureAuth
I0524 16:31:06.725674    3448 ubuntu.go:193] setting minikube options for container-runtime
I0524 16:31:06.726198    3448 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0524 16:31:06.737156    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:06.836369    3448 main.go:141] libmachine: Using SSH client type: native
I0524 16:31:06.836369    3448 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa3c0] 0x4acfa0 <nil>  [] 0s} 127.0.0.1 49443 <nil> <nil>}
I0524 16:31:06.836369    3448 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0524 16:31:06.956337    3448 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0524 16:31:06.956337    3448 ubuntu.go:71] root file system type: overlay
I0524 16:31:06.956337    3448 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0524 16:31:06.963836    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:07.064411    3448 main.go:141] libmachine: Using SSH client type: native
I0524 16:31:07.064411    3448 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa3c0] 0x4acfa0 <nil>  [] 0s} 127.0.0.1 49443 <nil> <nil>}
I0524 16:31:07.064411    3448 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0524 16:31:07.194096    3448 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0524 16:31:07.201976    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:07.310409    3448 main.go:141] libmachine: Using SSH client type: native
I0524 16:31:07.310409    3448 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa3c0] 0x4acfa0 <nil>  [] 0s} 127.0.0.1 49443 <nil> <nil>}
I0524 16:31:07.310409    3448 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0524 16:31:07.442841    3448 main.go:141] libmachine: SSH cmd err, output: <nil>:
I0524 16:31:07.442841    3448 machine.go:97] duration metric: took 2.0621246s to provisionDockerMachine
I0524 16:31:07.442841    3448 start.go:293] postStartSetup for "minikube" (driver="docker")
I0524 16:31:07.442841    3448 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0524 16:31:07.455040    3448 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0524 16:31:07.462419    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:07.573194    3448 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49443 SSHKeyPath:C:\Users\LEGION\.minikube\machines\minikube\id_rsa Username:docker}
I0524 16:31:07.687970    3448 ssh_runner.go:195] Run: cat /etc/os-release
I0524 16:31:07.692336    3448 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0524 16:31:07.692852    3448 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0524 16:31:07.692852    3448 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0524 16:31:07.692852    3448 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0524 16:31:07.692852    3448 filesync.go:126] Scanning C:\Users\LEGION\.minikube\addons for local assets ...
I0524 16:31:07.693183    3448 filesync.go:126] Scanning C:\Users\LEGION\.minikube\files for local assets ...
I0524 16:31:07.693183    3448 start.go:296] duration metric: took 250.3423ms for postStartSetup
I0524 16:31:07.698447    3448 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0524 16:31:07.708535    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:07.817514    3448 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49443 SSHKeyPath:C:\Users\LEGION\.minikube\machines\minikube\id_rsa Username:docker}
I0524 16:31:07.911899    3448 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0524 16:31:07.916116    3448 fix.go:56] duration metric: took 2.6735814s for fixHost
I0524 16:31:07.916116    3448 start.go:83] releasing machines lock for "minikube", held for 2.6735814s
I0524 16:31:07.923987    3448 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0524 16:31:08.049067    3448 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0524 16:31:08.057486    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:08.060686    3448 ssh_runner.go:195] Run: cat /version.json
I0524 16:31:08.068552    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:08.170384    3448 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49443 SSHKeyPath:C:\Users\LEGION\.minikube\machines\minikube\id_rsa Username:docker}
I0524 16:31:08.186193    3448 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49443 SSHKeyPath:C:\Users\LEGION\.minikube\machines\minikube\id_rsa Username:docker}
I0524 16:31:08.268353    3448 ssh_runner.go:195] Run: systemctl --version
I0524 16:31:08.529841    3448 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0524 16:31:08.545016    3448 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0524 16:31:08.552306    3448 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0524 16:31:08.563874    3448 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0524 16:31:08.571302    3448 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0524 16:31:08.571302    3448 start.go:494] detecting cgroup driver to use...
I0524 16:31:08.571302    3448 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0524 16:31:08.571302    3448 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0524 16:31:08.590710    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0524 16:31:08.604489    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0524 16:31:08.614052    3448 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0524 16:31:08.618239    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0524 16:31:08.632482    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0524 16:31:08.645625    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0524 16:31:08.659921    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0524 16:31:08.673973    3448 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0524 16:31:08.687220    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0524 16:31:08.700716    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0524 16:31:08.714774    3448 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0524 16:31:08.735904    3448 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0524 16:31:08.757227    3448 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0524 16:31:08.777530    3448 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 16:31:08.900838    3448 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0524 16:31:19.364193    3448 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.4633543s)
I0524 16:31:19.364193    3448 start.go:494] detecting cgroup driver to use...
I0524 16:31:19.364193    3448 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0524 16:31:19.379009    3448 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0524 16:31:19.389853    3448 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0524 16:31:19.405625    3448 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0524 16:31:19.416039    3448 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0524 16:31:19.441338    3448 ssh_runner.go:195] Run: which cri-dockerd
I0524 16:31:19.459018    3448 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0524 16:31:19.469506    3448 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0524 16:31:19.498100    3448 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0524 16:31:19.600240    3448 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0524 16:31:19.708576    3448 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0524 16:31:19.708576    3448 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0524 16:31:19.733649    3448 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 16:31:19.839936    3448 ssh_runner.go:195] Run: sudo systemctl restart docker
I0524 16:31:20.169200    3448 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0524 16:31:20.193498    3448 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0524 16:31:20.234897    3448 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0524 16:31:20.258212    3448 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0524 16:31:20.361461    3448 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0524 16:31:20.459035    3448 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 16:31:20.578743    3448 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0524 16:31:20.609332    3448 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0524 16:31:20.633428    3448 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 16:31:20.748639    3448 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0524 16:31:20.825399    3448 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0524 16:31:20.839415    3448 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0524 16:31:20.844209    3448 start.go:562] Will wait 60s for crictl version
I0524 16:31:20.861297    3448 ssh_runner.go:195] Run: which crictl
I0524 16:31:20.880827    3448 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0524 16:31:20.917040    3448 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0524 16:31:20.926048    3448 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0524 16:31:20.956320    3448 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0524 16:31:20.976043    3448 out.go:204] * Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0524 16:31:20.985554    3448 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0524 16:31:21.131777    3448 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0524 16:31:21.146477    3448 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0524 16:31:21.158761    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0524 16:31:21.271475    3448 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LEGION:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0524 16:31:21.271475    3448 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0524 16:31:21.284253    3448 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0524 16:31:21.366015    3448 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0524 16:31:21.366015    3448 docker.go:615] Images already preloaded, skipping extraction
I0524 16:31:21.375573    3448 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0524 16:31:21.393463    3448 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0524 16:31:21.393463    3448 cache_images.go:84] Images are preloaded, skipping loading
I0524 16:31:21.393463    3448 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0524 16:31:21.393463    3448 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0524 16:31:21.403153    3448 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0524 16:31:21.568176    3448 cni.go:84] Creating CNI manager for ""
I0524 16:31:21.568176    3448 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0524 16:31:21.568176    3448 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0524 16:31:21.568176    3448 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0524 16:31:21.568176    3448 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0524 16:31:21.583013    3448 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0524 16:31:21.662144    3448 binaries.go:44] Found k8s binaries, skipping transfer
I0524 16:31:21.677560    3448 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0524 16:31:21.755660    3448 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0524 16:31:21.775235    3448 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0524 16:31:21.789978    3448 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0524 16:31:21.878789    3448 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0524 16:31:21.896186    3448 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 16:31:22.001931    3448 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0524 16:31:22.013291    3448 certs.go:68] Setting up C:\Users\LEGION\.minikube\profiles\minikube for IP: 192.168.49.2
I0524 16:31:22.013291    3448 certs.go:194] generating shared ca certs ...
I0524 16:31:22.013291    3448 certs.go:226] acquiring lock for ca certs: {Name:mkf9531e524ec8f433524584e6fd652a01c91d8f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 16:31:22.014077    3448 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\LEGION\.minikube\ca.key
I0524 16:31:22.014077    3448 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\LEGION\.minikube\proxy-client-ca.key
I0524 16:31:22.014077    3448 certs.go:256] generating profile certs ...
I0524 16:31:22.014606    3448 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\LEGION\.minikube\profiles\minikube\client.key
I0524 16:31:22.014606    3448 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\LEGION\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0524 16:31:22.015125    3448 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\LEGION\.minikube\profiles\minikube\proxy-client.key
I0524 16:31:22.016203    3448 certs.go:484] found cert: C:\Users\LEGION\.minikube\certs\ca-key.pem (1675 bytes)
I0524 16:31:22.016203    3448 certs.go:484] found cert: C:\Users\LEGION\.minikube\certs\ca.pem (1078 bytes)
I0524 16:31:22.016203    3448 certs.go:484] found cert: C:\Users\LEGION\.minikube\certs\cert.pem (1119 bytes)
I0524 16:31:22.016729    3448 certs.go:484] found cert: C:\Users\LEGION\.minikube\certs\key.pem (1675 bytes)
I0524 16:31:22.017257    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0524 16:31:22.038145    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0524 16:31:22.060253    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0524 16:31:22.080943    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0524 16:31:22.106940    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0524 16:31:22.132206    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0524 16:31:22.153468    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0524 16:31:22.183233    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0524 16:31:22.208302    3448 ssh_runner.go:362] scp C:\Users\LEGION\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0524 16:31:22.233233    3448 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (742 bytes)
I0524 16:31:22.270536    3448 ssh_runner.go:195] Run: openssl version
I0524 16:31:22.307550    3448 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0524 16:31:22.337724    3448 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0524 16:31:22.344151    3448 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 20 06:19 /usr/share/ca-certificates/minikubeCA.pem
I0524 16:31:22.359856    3448 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0524 16:31:22.384340    3448 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0524 16:31:22.410282    3448 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0524 16:31:22.427529    3448 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0524 16:31:22.446977    3448 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0524 16:31:22.471632    3448 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0524 16:31:22.494324    3448 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0524 16:31:22.517313    3448 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0524 16:31:22.542765    3448 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0524 16:31:22.550027    3448 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3500 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\LEGION:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0524 16:31:22.561285    3448 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0524 16:31:22.595310    3448 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0524 16:31:22.605477    3448 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0524 16:31:22.605477    3448 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0524 16:31:22.605477    3448 kubeadm.go:587] restartPrimaryControlPlane start ...
I0524 16:31:22.627792    3448 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0524 16:31:22.636180    3448 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0524 16:31:22.647268    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0524 16:31:22.758938    3448 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:49447"
I0524 16:31:22.776850    3448 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0524 16:31:22.785655    3448 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0524 16:31:22.786179    3448 kubeadm.go:591] duration metric: took 180.7015ms to restartPrimaryControlPlane
I0524 16:31:22.786179    3448 kubeadm.go:393] duration metric: took 236.1521ms to StartCluster
I0524 16:31:22.786179    3448 settings.go:142] acquiring lock: {Name:mkc37833be3ae4786b4a21349166e339f0ca8201 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 16:31:22.786179    3448 settings.go:150] Updating kubeconfig:  C:\Users\LEGION\.kube\config
I0524 16:31:22.787752    3448 lock.go:35] WriteFile acquiring C:\Users\LEGION\.kube\config: {Name:mkf889516bb7855c3f80bbcdf6c23e4385e7730f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0524 16:31:22.788278    3448 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0524 16:31:22.788815    3448 out.go:177] * Verifying Kubernetes components...
I0524 16:31:22.788278    3448 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0524 16:31:22.788278    3448 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0524 16:31:22.788815    3448 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0524 16:31:22.788815    3448 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0524 16:31:22.788815    3448 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0524 16:31:22.788815    3448 addons.go:243] addon storage-provisioner should already be in state true
I0524 16:31:22.788815    3448 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0524 16:31:22.789355    3448 host.go:66] Checking if "minikube" exists ...
I0524 16:31:22.806453    3448 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0524 16:31:22.812763    3448 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 16:31:22.813832    3448 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 16:31:22.935104    3448 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0524 16:31:22.941043    3448 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0524 16:31:22.941584    3448 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0524 16:31:22.941584    3448 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0524 16:31:22.953455    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:22.956449    3448 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0524 16:31:22.956449    3448 addons.go:243] addon default-storageclass should already be in state true
I0524 16:31:22.956449    3448 host.go:66] Checking if "minikube" exists ...
I0524 16:31:22.959607    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0524 16:31:22.978799    3448 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0524 16:31:23.077100    3448 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49443 SSHKeyPath:C:\Users\LEGION\.minikube\machines\minikube\id_rsa Username:docker}
I0524 16:31:23.094686    3448 api_server.go:52] waiting for apiserver process to appear ...
I0524 16:31:23.112788    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:23.124403    3448 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0524 16:31:23.124403    3448 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0524 16:31:23.135144    3448 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0524 16:31:23.204783    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0524 16:31:23.261007    3448 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49443 SSHKeyPath:C:\Users\LEGION\.minikube\machines\minikube\id_rsa Username:docker}
W0524 16:31:23.267275    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:23.267275    3448 retry.go:31] will retry after 355.038046ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:23.381910    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:23.427803    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:23.427803    3448 retry.go:31] will retry after 210.988662ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:23.609170    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:23.641359    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0524 16:31:23.655759    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:23.689243    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:23.689243    3448 retry.go:31] will retry after 326.31384ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0524 16:31:23.708096    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:23.708096    3448 retry.go:31] will retry after 255.405103ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:23.987834    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0524 16:31:24.031809    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:24.032851    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:24.032851    3448 retry.go:31] will retry after 321.8986ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0524 16:31:24.074385    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:24.074385    3448 retry.go:31] will retry after 716.148432ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:24.122398    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:24.367287    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:24.414632    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:24.414632    3448 retry.go:31] will retry after 790.756633ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:24.614926    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:24.817277    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:24.862407    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:24.862407    3448 retry.go:31] will retry after 835.789754ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:25.116396    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:25.223608    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:25.276390    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:25.276390    3448 retry.go:31] will retry after 1.093765109s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:25.619527    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:25.711567    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:25.760183    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:25.760708    3448 retry.go:31] will retry after 702.5121ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:26.117937    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:26.393135    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:26.437671    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:26.437671    3448 retry.go:31] will retry after 1.197033934s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:26.485807    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:26.532045    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:26.532045    3448 retry.go:31] will retry after 1.960037825s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:26.608969    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:27.121212    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:27.613388    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:27.659687    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:27.712748    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:27.712748    3448 retry.go:31] will retry after 4.14134825s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:28.112626    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:28.517984    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:28.569801    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:28.569801    3448 retry.go:31] will retry after 4.234337605s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:28.609526    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:29.122082    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:29.612788    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:30.109491    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:30.621129    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:31.120556    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:31.615561    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:31.881024    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:31.953441    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:31.953441    3448 retry.go:31] will retry after 4.083619363s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:32.109971    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:32.622818    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:32.824333    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:32.874472    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:32.874472    3448 retry.go:31] will retry after 5.806889085s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:33.121448    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:33.620560    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:34.124496    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:34.616685    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:35.112398    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:35.616358    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:36.065469    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0524 16:31:36.124399    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0524 16:31:36.131341    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:36.131341    3448 retry.go:31] will retry after 8.057561062s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:36.616170    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:37.112171    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:37.611070    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:38.121971    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:38.621084    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:38.697813    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:38.754965    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:38.754965    3448 retry.go:31] will retry after 8.036771643s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:39.112114    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:39.613604    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:40.123387    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:40.609690    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:41.121053    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:41.615775    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:42.114057    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:42.621512    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:43.115845    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:43.609423    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:44.113248    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:44.202478    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0524 16:31:44.257171    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:44.257171    3448 retry.go:31] will retry after 9.547643459s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:44.617175    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:45.111638    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:45.623805    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:46.113253    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:46.621868    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:46.804359    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0524 16:31:46.856030    3448 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:46.856030    3448 retry.go:31] will retry after 10.51928743s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0524 16:31:47.112521    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:47.614536    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:48.121491    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:48.616935    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:49.115776    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:49.612892    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:50.109118    3448 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0524 16:31:50.118413    3448 api_server.go:72] duration metric: took 27.330135s to wait for apiserver process to appear ...
I0524 16:31:50.118413    3448 api_server.go:88] waiting for apiserver healthz status ...
I0524 16:31:50.118413    3448 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49447/healthz ...
I0524 16:31:51.063374    3448 api_server.go:279] https://127.0.0.1:49447/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0524 16:31:51.063374    3448 api_server.go:103] status: https://127.0.0.1:49447/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0524 16:31:51.063374    3448 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49447/healthz ...
I0524 16:31:51.154581    3448 api_server.go:279] https://127.0.0.1:49447/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0524 16:31:51.154581    3448 api_server.go:103] status: https://127.0.0.1:49447/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0524 16:31:51.154581    3448 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49447/healthz ...
I0524 16:31:51.159343    3448 api_server.go:279] https://127.0.0.1:49447/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0524 16:31:51.159343    3448 api_server.go:103] status: https://127.0.0.1:49447/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0524 16:31:51.622146    3448 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49447/healthz ...
I0524 16:31:51.627953    3448 api_server.go:279] https://127.0.0.1:49447/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0524 16:31:51.627953    3448 api_server.go:103] status: https://127.0.0.1:49447/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0524 16:31:52.119435    3448 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49447/healthz ...
I0524 16:31:52.124137    3448 api_server.go:279] https://127.0.0.1:49447/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0524 16:31:52.124137    3448 api_server.go:103] status: https://127.0.0.1:49447/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0524 16:31:52.632957    3448 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49447/healthz ...
I0524 16:31:52.638252    3448 api_server.go:279] https://127.0.0.1:49447/healthz returned 200:
ok
I0524 16:31:52.644560    3448 api_server.go:141] control plane version: v1.30.0
I0524 16:31:52.644560    3448 api_server.go:131] duration metric: took 2.5261472s to wait for apiserver health ...
I0524 16:31:52.644560    3448 system_pods.go:43] waiting for kube-system pods to appear ...
I0524 16:31:52.654534    3448 system_pods.go:59] 7 kube-system pods found
I0524 16:31:52.654534    3448 system_pods.go:61] "coredns-7db6d8ff4d-8bz65" [bc2ca2b3-22d7-4111-8c12-a860de9a9dc2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0524 16:31:52.654534    3448 system_pods.go:61] "etcd-minikube" [baf2454a-91db-4e40-a247-214abe97c924] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0524 16:31:52.654534    3448 system_pods.go:61] "kube-apiserver-minikube" [00497bba-f199-4b20-b793-c2f911ad2dac] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0524 16:31:52.654534    3448 system_pods.go:61] "kube-controller-manager-minikube" [9b448314-f8be-4897-afb0-369f12ac39b8] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0524 16:31:52.654534    3448 system_pods.go:61] "kube-proxy-rf74s" [609982f3-0846-435e-8de0-a294d0ca2962] Running
I0524 16:31:52.654534    3448 system_pods.go:61] "kube-scheduler-minikube" [8c552148-6dc2-4b84-99d0-0599538e3d67] Running
I0524 16:31:52.654534    3448 system_pods.go:61] "storage-provisioner" [d7b03cd1-3b08-4a0c-8959-936e45c39eff] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0524 16:31:52.654534    3448 system_pods.go:74] duration metric: took 9.9737ms to wait for pod list to return data ...
I0524 16:31:52.654534    3448 kubeadm.go:576] duration metric: took 29.8662559s to wait for: map[apiserver:true system_pods:true]
I0524 16:31:52.654534    3448 node_conditions.go:102] verifying NodePressure condition ...
I0524 16:31:52.657342    3448 node_conditions.go:122] node storage ephemeral capacity is 263112772Ki
I0524 16:31:52.657342    3448 node_conditions.go:123] node cpu capacity is 16
I0524 16:31:52.657342    3448 node_conditions.go:105] duration metric: took 2.8084ms to run NodePressure ...
I0524 16:31:52.657342    3448 start.go:240] waiting for startup goroutines ...
I0524 16:31:53.822949    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0524 16:31:57.395039    3448 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0524 16:31:57.814011    3448 out.go:177] * Enabled addons: default-storageclass, storage-provisioner
I0524 16:31:57.814534    3448 addons.go:505] duration metric: took 35.0262561s for enable addons: enabled=[default-storageclass storage-provisioner]
I0524 16:31:57.815057    3448 start.go:245] waiting for cluster config update ...
I0524 16:31:57.815057    3448 start.go:254] writing updated cluster config ...
I0524 16:31:57.831535    3448 ssh_runner.go:195] Run: rm -f paused
I0524 16:31:57.986889    3448 start.go:600] kubectl: 1.30.0, cluster: 1.30.0 (minor skew: 0)
I0524 16:31:57.989035    3448 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 24 10:46:19 minikube dockerd[6751]: time="2024-05-24T10:46:19.716704182Z" level=info msg="Daemon has completed initialization"
May 24 10:46:19 minikube dockerd[6751]: time="2024-05-24T10:46:19.759315809Z" level=info msg="API listen on /var/run/docker.sock"
May 24 10:46:19 minikube dockerd[6751]: time="2024-05-24T10:46:19.759334735Z" level=info msg="API listen on [::]:2376"
May 24 10:46:19 minikube systemd[1]: Started Docker Application Container Engine.
May 24 10:46:19 minikube systemd[1]: Stopping Docker Application Container Engine...
May 24 10:46:19 minikube dockerd[6751]: time="2024-05-24T10:46:19.848945582Z" level=info msg="Processing signal 'terminated'"
May 24 10:46:19 minikube dockerd[6751]: time="2024-05-24T10:46:19.850343468Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
May 24 10:46:19 minikube dockerd[6751]: time="2024-05-24T10:46:19.850983408Z" level=info msg="Daemon shutdown complete"
May 24 10:46:19 minikube systemd[1]: docker.service: Deactivated successfully.
May 24 10:46:19 minikube systemd[1]: Stopped Docker Application Container Engine.
May 24 10:46:19 minikube systemd[1]: Starting Docker Application Container Engine...
May 24 10:46:19 minikube dockerd[7005]: time="2024-05-24T10:46:19.895377778Z" level=info msg="Starting up"
May 24 10:46:19 minikube dockerd[7005]: time="2024-05-24T10:46:19.915115021Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 24 10:46:19 minikube dockerd[7005]: time="2024-05-24T10:46:19.934324636Z" level=info msg="Loading containers: start."
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.053645699Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.088261550Z" level=info msg="Loading containers: done."
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.111237250Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.111269042Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.111274863Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.111279472Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.111298419Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.111372121Z" level=info msg="Daemon has completed initialization"
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.152323028Z" level=info msg="API listen on /var/run/docker.sock"
May 24 10:46:20 minikube dockerd[7005]: time="2024-05-24T10:46:20.152373324Z" level=info msg="API listen on [::]:2376"
May 24 10:46:20 minikube systemd[1]: Started Docker Application Container Engine.
May 24 10:46:20 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
May 24 10:46:20 minikube systemd[1]: cri-docker.service: Deactivated successfully.
May 24 10:46:20 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
May 24 10:46:20 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Start docker client with request timeout 0s"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Loaded network plugin cni"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Docker cri networking managed by network plugin cni"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Setting cgroupDriver cgroupfs"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 24 10:46:20 minikube cri-dockerd[7259]: time="2024-05-24T10:46:20Z" level=info msg="Start cri-dockerd grpc backend"
May 24 10:46:20 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-8bz65_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3bd23dd6e79538dc29f9bd7f5ceeba0ecdcfecf2528a23459d2c8496c3e50dfd\""
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-8bz65_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0d4338f204a480fa6c4352dc370d6a615f1b2950e145bfc36d1696f4afc758b8\""
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-8bz65_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d55b70f8971a3623cb74b871a4a8f214881519f94fbaa9a95ef4b3fca77523da\""
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cdd79ce1b982f3942167baf1f3cb3bb55befa03f2079ccf379978bfe3dbe35f1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f351d005704b0780b5de6490a5cb5ed6f5b7ee2b37e0ca6192763584c74949a5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fa41e0edd565e15aa717381f188fbec2a1bff6598d555448cfd2ef5c41968900/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aab641118f19b7d6b5ed30cc761ec275829839a9c8278c4bafa02cd2ef4d7b9a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/33e4df8a0e739ff12c52c39f4c28509eebf47d7b0be316a2ce7fd69e2e7ff762/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 24 10:46:21 minikube cri-dockerd[7259]: time="2024-05-24T10:46:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/35ff63655fd41fdcee0ca67b7172770e48f7e2502ea6a6291e55b1a66e7f95c5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 24 10:46:23 minikube cri-dockerd[7259]: time="2024-05-24T10:46:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a205cadfcfb3c2365cf5a8c7f538b15a8522439a0dd76e1acefa8da250a760b7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 24 10:46:36 minikube dockerd[7005]: time="2024-05-24T10:46:36.772657467Z" level=info msg="ignoring event" container=6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 24 10:56:44 minikube cri-dockerd[7259]: time="2024-05-24T10:56:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d7f979e6bbc7721e9456070eef8f94ae444c48f4dd53332edfbb7c8894295fa2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 24 10:56:48 minikube dockerd[7005]: time="2024-05-24T10:56:48.309042380Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=02b3efc518fd8472 traceID=60adda58fbc995cf6574e06ee15cb612
May 24 10:56:48 minikube dockerd[7005]: time="2024-05-24T10:56:48.309170649Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 10:57:03 minikube dockerd[7005]: time="2024-05-24T10:57:03.661307932Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b402ff2d553e956c traceID=0fd243f430dc5b97378d068ab375cbf4
May 24 10:57:03 minikube dockerd[7005]: time="2024-05-24T10:57:03.661391312Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 10:57:34 minikube dockerd[7005]: time="2024-05-24T10:57:34.201651415Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=8dc5e11bf72ae30a traceID=7b92866de214b1478a166a92c768ff81
May 24 10:57:34 minikube dockerd[7005]: time="2024-05-24T10:57:34.201754494Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 24 10:58:32 minikube dockerd[7005]: time="2024-05-24T10:58:32.200812351Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=050dc601ffd53cdb traceID=6eab8d4ce104d1783eaa53ef3b3adac6
May 24 10:58:32 minikube dockerd[7005]: time="2024-05-24T10:58:32.200887305Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
5c0a66e131a8a       6e38f40d628db       11 minutes ago      Running             storage-provisioner       8                   aab641118f19b       storage-provisioner
e80b90768e252       c7aad43836fa5       12 minutes ago      Running             kube-controller-manager   5                   cdd79ce1b982f       kube-controller-manager-minikube
524642df46872       c42f13656d0b2       12 minutes ago      Running             kube-apiserver            4                   a205cadfcfb3c       kube-apiserver-minikube
9e72948cfb8e8       3861cfcd7c04c       13 minutes ago      Running             etcd                      4                   f351d005704b0       etcd-minikube
ae2ce278fe066       a0bf559e280cf       13 minutes ago      Running             kube-proxy                4                   33e4df8a0e739       kube-proxy-rf74s
6e9d5eb9fd8be       6e38f40d628db       13 minutes ago      Exited              storage-provisioner       7                   aab641118f19b       storage-provisioner
22e9507decbed       cbb01a7bd410d       13 minutes ago      Running             coredns                   4                   35ff63655fd41       coredns-7db6d8ff4d-8bz65
23ae5a62f921c       259c8277fcbbc       13 minutes ago      Running             kube-scheduler            4                   fa41e0edd565e       kube-scheduler-minikube
11bce3018f63e       c7aad43836fa5       13 minutes ago      Exited              kube-controller-manager   4                   8107fb47be944       kube-controller-manager-minikube
dbf50bd5cbbf9       c42f13656d0b2       13 minutes ago      Exited              kube-apiserver            3                   e30b017c711b8       kube-apiserver-minikube
65fab81abda1e       cbb01a7bd410d       14 minutes ago      Exited              coredns                   3                   3bd23dd6e7953       coredns-7db6d8ff4d-8bz65
96d177a66222d       259c8277fcbbc       14 minutes ago      Exited              kube-scheduler            3                   3609afe85deb5       kube-scheduler-minikube
0f5a1e2f9f0ad       3861cfcd7c04c       14 minutes ago      Exited              etcd                      3                   cc618499806d2       etcd-minikube
df597f6fb68a6       a0bf559e280cf       14 minutes ago      Exited              kube-proxy                3                   54742bd4fe090       kube-proxy-rf74s


==> coredns [22e9507decbe] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:36365 - 43394 "HINFO IN 4172823317955223666.3344579863066475725. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.0637907s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [65fab81abda1] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:46286 - 28738 "HINFO IN 8126565345335969284.8878481614726410518. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.066430214s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_23T17_22_42_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 23 May 2024 11:37:39 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 24 May 2024 10:59:39 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 24 May 2024 10:58:38 +0000   Thu, 23 May 2024 11:37:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 24 May 2024 10:58:38 +0000   Thu, 23 May 2024 11:37:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 24 May 2024 10:58:38 +0000   Thu, 23 May 2024 11:37:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 24 May 2024 10:58:38 +0000   Thu, 23 May 2024 11:37:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7027112Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7027112Ki
  pods:               110
System Info:
  Machine ID:                 fc21810a97c94590bc20044755a14b48
  System UUID:                fc21810a97c94590bc20044755a14b48
  Boot ID:                    ebde28c1-6cd8-43da-a786-0d3e17ededae
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     mlops-minikube-56fff5959-572ct      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m56s
  kube-system                 coredns-7db6d8ff4d-8bz65            100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     23h
  kube-system                 etcd-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         23h
  kube-system                 kube-apiserver-minikube             250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 kube-controller-manager-minikube    200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 kube-proxy-rf74s                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 kube-scheduler-minikube             100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 9h                 kube-proxy
  Normal   Starting                 12m                kube-proxy
  Normal   Starting                 13m                kube-proxy
  Normal   Starting                 16m                kube-proxy
  Normal   Starting                 23h                kube-proxy
  Normal   Starting                 23h                kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  23h                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  23h                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    23h                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     23h                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           23h                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  ContainerGCFailed        9h                 kubelet          rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
  Normal   NodeNotReady             9h                 kubelet          Node minikube status is now: NodeNotReady
  Normal   RegisteredNode           9h                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 16m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  16m (x8 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    16m (x8 over 16m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     16m (x7 over 16m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  16m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           16m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode           12m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.001818] FS-Cache: Duplicate cookie detected
[  +0.000316] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000300] FS-Cache: O-cookie d=00000000f0e07e23{9P.session} n=00000000f687dba5
[  +0.000362] FS-Cache: O-key=[10] '34323934393337333635'
[  +0.000224] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000279] FS-Cache: N-cookie d=00000000f0e07e23{9P.session} n=00000000f3e29e73
[  +0.000381] FS-Cache: N-key=[10] '34323934393337333635'
[  +0.355530] /sbin/ldconfig:
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.204838] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.008580] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Katmandu not found. Is the tzdata package installed?
[  +0.245582] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000630] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000512] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000586] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001103] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000427] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000500] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000573] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.421767] /sbin/ldconfig:
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.018667] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000959] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.000851] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.001882] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.002974] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.000779] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.011824] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Katmandu not found. Is the tzdata package installed?
[  +0.078378] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001226] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000868] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000782] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001383] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000681] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000576] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000654] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.262684] /sbin/ldconfig:
[  +0.000006] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.088136] /sbin/ldconfig.real:
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.028128] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000778] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000654] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000118] netlink: 'init': attribute type 4 has an invalid length.
[  +0.000507] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001471] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000578] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000539] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000581] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.088654] new mount options do not match the existing superblock, will be ignored
[  +0.215827] Failed to connect to bus: No such file or directory
[  +0.168873] systemd-journald[54]: File /var/log/journal/c0890a6c5f9f5d5818ff6f936464fe10/system.journal corrupted or uncleanly shut down, renaming and replacing.


==> etcd [0f5a1e2f9f0a] <==
{"level":"warn","ts":"2024-05-24T10:45:33.672472Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-24T10:45:33.672572Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-05-24T10:45:33.672645Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-05-24T10:45:33.67268Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-24T10:45:33.672699Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-24T10:45:33.672725Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-24T10:45:33.673482Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-05-24T10:45:33.673631Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-05-24T10:45:33.675357Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.002427ms"}
{"level":"info","ts":"2024-05-24T10:45:33.758959Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-05-24T10:45:33.77795Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":7352}
{"level":"info","ts":"2024-05-24T10:45:33.778111Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-05-24T10:45:33.778158Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2024-05-24T10:45:33.778182Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 4, commit: 7352, applied: 0, lastindex: 7352, lastterm: 4]"}
{"level":"warn","ts":"2024-05-24T10:45:33.779251Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-24T10:45:33.780105Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":5326}
{"level":"info","ts":"2024-05-24T10:45:33.78125Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":5950}
{"level":"info","ts":"2024-05-24T10:45:33.782743Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-24T10:45:33.784773Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-05-24T10:45:33.785137Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-24T10:45:33.785168Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-05-24T10:45:33.785256Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-05-24T10:45:33.785365Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T10:45:33.785423Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T10:45:33.785431Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T10:45:33.785646Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-05-24T10:45:33.785727Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-24T10:45:33.785811Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-24T10:45:33.785846Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-24T10:45:33.855951Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-24T10:45:33.856026Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-24T10:45:33.856048Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-24T10:45:33.856275Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-24T10:45:33.856319Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-24T10:45:35.278082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2024-05-24T10:45:35.278167Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2024-05-24T10:45:35.278188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-05-24T10:45:35.278215Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2024-05-24T10:45:35.278225Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-05-24T10:45:35.278233Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2024-05-24T10:45:35.27824Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-05-24T10:45:35.279382Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-24T10:45:35.279418Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-24T10:45:35.279395Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-24T10:45:35.279736Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-24T10:45:35.279755Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-24T10:45:35.281908Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-05-24T10:45:35.281934Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-05-24T10:46:08.9605Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-05-24T10:46:08.960614Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-05-24T10:46:08.960793Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-24T10:46:08.960935Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-24T10:46:09.057445Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-24T10:46:09.05752Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-05-24T10:46:09.057591Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-24T10:46:09.062877Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-24T10:46:09.063185Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-24T10:46:09.063264Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [9e72948cfb8e] <==
{"level":"info","ts":"2024-05-24T10:46:36.871555Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-05-24T10:46:36.871671Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-05-24T10:46:36.871707Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-24T10:46:36.871715Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-24T10:46:36.87174Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-24T10:46:36.872358Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-05-24T10:46:36.872469Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-05-24T10:46:36.874126Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.492062ms"}
{"level":"info","ts":"2024-05-24T10:46:36.886583Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-05-24T10:46:36.903007Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":7442}
{"level":"info","ts":"2024-05-24T10:46:36.904069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-05-24T10:46:36.904132Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2024-05-24T10:46:36.904144Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 5, commit: 7442, applied: 0, lastindex: 7442, lastterm: 5]"}
{"level":"warn","ts":"2024-05-24T10:46:36.905276Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-24T10:46:36.906064Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":5326}
{"level":"info","ts":"2024-05-24T10:46:36.907777Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":6033}
{"level":"info","ts":"2024-05-24T10:46:36.90916Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-24T10:46:36.911419Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-05-24T10:46:36.911982Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-24T10:46:36.912026Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-05-24T10:46:36.9122Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-05-24T10:46:36.912372Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-05-24T10:46:36.912469Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-24T10:46:36.912611Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-24T10:46:36.912704Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-24T10:46:36.914815Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T10:46:36.914921Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T10:46:36.91495Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-24T10:46:36.917196Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-24T10:46:36.917385Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-24T10:46:36.917531Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-24T10:46:36.917526Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-24T10:46:36.917559Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-24T10:46:38.105431Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2024-05-24T10:46:38.105534Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2024-05-24T10:46:38.105556Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-05-24T10:46:38.105568Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2024-05-24T10:46:38.105574Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-05-24T10:46:38.105582Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-05-24T10:46:38.105588Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-05-24T10:46:38.108695Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-24T10:46:38.108708Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-24T10:46:38.108736Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-24T10:46:38.108955Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-24T10:46:38.109022Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-24T10:46:38.110087Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-05-24T10:46:38.110096Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"warn","ts":"2024-05-24T10:50:58.481932Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"513.638504ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029388705000754 > lease_revoke:<id:70cc8faa35b26cfc>","response":"size:29"}
{"level":"warn","ts":"2024-05-24T10:50:58.501781Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"552.423094ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-05-24T10:50:58.503938Z","caller":"traceutil/trace.go:171","msg":"trace[1875530780] transaction","detail":"{read_only:false; response_revision:6255; number_of_response:1; }","duration":"158.022437ms","start":"2024-05-24T10:50:58.343623Z","end":"2024-05-24T10:50:58.501645Z","steps":["trace[1875530780] 'process raft request'  (duration: 153.613591ms)"],"step_count":1}
{"level":"info","ts":"2024-05-24T10:50:58.503912Z","caller":"traceutil/trace.go:171","msg":"trace[526977120] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:6254; }","duration":"556.666766ms","start":"2024-05-24T10:50:57.945245Z","end":"2024-05-24T10:50:58.501912Z","steps":["trace[526977120] 'agreement among raft nodes before linearized reading'  (duration: 552.14792ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-24T10:50:58.517243Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-24T10:50:57.945231Z","time spent":"571.096043ms","remote":"127.0.0.1:46336","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":31,"request content":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true "}
{"level":"info","ts":"2024-05-24T10:50:58.506074Z","caller":"traceutil/trace.go:171","msg":"trace[2105816792] linearizableReadLoop","detail":"{readStateIndex:7719; appliedIndex:7718; }","duration":"551.954494ms","start":"2024-05-24T10:50:57.94531Z","end":"2024-05-24T10:50:58.497264Z","steps":["trace[2105816792] 'read index received'  (duration: 617.268s)","trace[2105816792] 'applied index is now lower than readState.Index'  (duration: 551.332928ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-24T10:50:58.543426Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.20872ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"warn","ts":"2024-05-24T10:50:58.54349Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"197.967648ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-24T10:50:58.543562Z","caller":"traceutil/trace.go:171","msg":"trace[1217923555] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:6255; }","duration":"200.374942ms","start":"2024-05-24T10:50:58.343119Z","end":"2024-05-24T10:50:58.543494Z","steps":["trace[1217923555] 'agreement among raft nodes before linearized reading'  (duration: 199.941872ms)"],"step_count":1}
{"level":"info","ts":"2024-05-24T10:50:58.543554Z","caller":"traceutil/trace.go:171","msg":"trace[174123654] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:6255; }","duration":"198.022614ms","start":"2024-05-24T10:50:58.34549Z","end":"2024-05-24T10:50:58.543513Z","steps":["trace[174123654] 'agreement among raft nodes before linearized reading'  (duration: 197.603706ms)"],"step_count":1}
{"level":"info","ts":"2024-05-24T10:56:50.081166Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6296}
{"level":"info","ts":"2024-05-24T10:56:50.218557Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6296,"took":"136.293807ms","hash":4118858722,"current-db-size-bytes":2998272,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1675264,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-05-24T10:56:50.218645Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4118858722,"revision":6296,"compact-revision":5326}


==> kernel <==
 10:59:40 up  4:54,  0 users,  load average: 0.11, 0.32, 0.34
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [524642df4687] <==
I0524 10:46:50.991564       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0524 10:46:50.991578       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0524 10:46:50.991636       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0524 10:46:50.991651       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0524 10:46:50.991691       1 aggregator.go:163] waiting for initial CRD sync...
I0524 10:46:50.991877       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0524 10:46:50.991956       1 controller.go:116] Starting legacy_token_tracking_controller
I0524 10:46:50.991962       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0524 10:46:50.992011       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0524 10:46:50.992036       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0524 10:46:50.992051       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0524 10:46:50.992056       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0524 10:46:50.992073       1 controller.go:78] Starting OpenAPI AggregationController
I0524 10:46:50.992093       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0524 10:46:50.992097       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0524 10:46:50.992140       1 controller.go:139] Starting OpenAPI controller
I0524 10:46:50.992191       1 controller.go:87] Starting OpenAPI V3 controller
I0524 10:46:50.992267       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0524 10:46:50.992449       1 crd_finalizer.go:266] Starting CRDFinalizer
I0524 10:46:50.991879       1 available_controller.go:423] Starting AvailableConditionController
I0524 10:46:50.992465       1 naming_controller.go:291] Starting NamingConditionController
I0524 10:46:50.992465       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0524 10:46:50.992477       1 establishing_controller.go:76] Starting EstablishingController
I0524 10:46:50.992500       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0524 10:46:50.992514       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0524 10:46:51.051759       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0524 10:46:51.142053       1 shared_informer.go:320] Caches are synced for node_authorizer
I0524 10:46:51.142170       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0524 10:46:51.142187       1 policy_source.go:224] refreshing policies
I0524 10:46:51.142204       1 shared_informer.go:320] Caches are synced for configmaps
I0524 10:46:51.143242       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0524 10:46:51.143298       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0524 10:46:51.143336       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0524 10:46:51.143348       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0524 10:46:51.144068       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0524 10:46:51.144348       1 aggregator.go:165] initial CRD sync complete...
I0524 10:46:51.144366       1 autoregister_controller.go:141] Starting autoregister controller
I0524 10:46:51.144371       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0524 10:46:51.144377       1 cache.go:39] Caches are synced for autoregister controller
I0524 10:46:51.143348       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0524 10:46:51.158237       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0524 10:46:51.160547       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0524 10:46:51.163101       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0524 10:46:51.994767       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0524 10:47:10.462296       1 controller.go:615] quota admission added evaluator for: endpoints
I0524 10:47:10.492442       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0524 10:50:45.473400       1 trace.go:236] Trace[481453316]: "Update" accept:application/json, */*,audit-id:dad869e4-e9ac-4271-a8b5-af00938afaf0,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (24-May-2024 10:50:44.864) (total time: 506ms):
Trace[481453316]: ["GuaranteedUpdate etcd3" audit-id:dad869e4-e9ac-4271-a8b5-af00938afaf0,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 505ms (10:50:44.865)
Trace[481453316]:  ---"About to Encode" 379ms (10:50:45.245)
Trace[481453316]:  ---"Txn call completed" 124ms (10:50:45.369)]
Trace[481453316]: [506.036814ms] [506.036814ms] END
I0524 10:50:56.655529       1 trace.go:236] Trace[477493545]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a0752703-f2aa-4697-bd44-e5fcaa13ecff,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (24-May-2024 10:50:55.848) (total time: 805ms):
Trace[477493545]: ---"limitedReadBody succeeded" len:604 595ms (10:50:56.443)
Trace[477493545]: [805.668823ms] [805.668823ms] END
I0524 10:50:58.546671       1 trace.go:236] Trace[800780752]: "Get" accept:application/json, */*,audit-id:6549c437-0673-4aa7-941f-3d24be8ebeab,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (24-May-2024 10:50:57.751) (total time: 795ms):
Trace[800780752]: ---"About to write a response" 794ms (10:50:58.546)
Trace[800780752]: [795.418821ms] [795.418821ms] END
I0524 10:56:43.331837       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0524 10:56:43.368410       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0524 10:57:09.038426       1 alloc.go:330] "allocated clusterIPs" service="default/mlops-minikube" clusterIPs={"IPv4":"10.108.218.41"}


==> kube-apiserver [dbf50bd5cbbf] <==
W0524 10:46:14.610355       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.615957       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.634504       1 logging.go:59] [core] [Channel #2 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.637988       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.660564       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.704775       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.716832       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.732069       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.744107       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.767017       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.768297       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.845097       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.893084       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:14.921215       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.254116       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.408024       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.536837       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.540182       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.549213       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.627505       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.669413       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.714316       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.736879       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.786794       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.829746       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.850759       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.932547       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.940288       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.950122       1 logging.go:59] [core] [Channel #16 SubChannel #17] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:17.976591       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.017085       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.027889       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.085846       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.095820       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.111072       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.118811       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.205172       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.206413       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.220042       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.232107       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.237873       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.254322       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.264348       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.398456       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.422158       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.491916       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.537333       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.564174       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.565500       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.589817       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.592279       1 logging.go:59] [core] [Channel #6 SubChannel #7] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.685382       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.744362       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.752021       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.785921       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.798801       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.973774       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.977179       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.977287       1 logging.go:59] [core] [Channel #1 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0524 10:46:18.980774       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [11bce3018f63] <==


==> kube-controller-manager [e80b90768e25] <==
I0524 10:47:10.450849       1 shared_informer.go:320] Caches are synced for GC
I0524 10:47:10.450879       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0524 10:47:10.451020       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0524 10:47:10.451036       1 shared_informer.go:320] Caches are synced for endpoint
I0524 10:47:10.451040       1 shared_informer.go:320] Caches are synced for taint
I0524 10:47:10.451024       1 shared_informer.go:320] Caches are synced for TTL
I0524 10:47:10.451202       1 shared_informer.go:320] Caches are synced for namespace
I0524 10:47:10.451211       1 shared_informer.go:320] Caches are synced for ReplicationController
I0524 10:47:10.451295       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0524 10:47:10.451383       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0524 10:47:10.451424       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0524 10:47:10.454658       1 shared_informer.go:320] Caches are synced for crt configmap
I0524 10:47:10.455810       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0524 10:47:10.458151       1 shared_informer.go:320] Caches are synced for job
I0524 10:47:10.462723       1 shared_informer.go:320] Caches are synced for node
I0524 10:47:10.462930       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0524 10:47:10.463003       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0524 10:47:10.463023       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0524 10:47:10.463029       1 shared_informer.go:320] Caches are synced for cidrallocator
I0524 10:47:10.466438       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0524 10:47:10.467637       1 shared_informer.go:320] Caches are synced for daemon sets
I0524 10:47:10.479487       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0524 10:47:10.479629       1 shared_informer.go:320] Caches are synced for cronjob
I0524 10:47:10.481880       1 shared_informer.go:320] Caches are synced for PV protection
I0524 10:47:10.483040       1 shared_informer.go:320] Caches are synced for TTL after finished
I0524 10:47:10.484190       1 shared_informer.go:320] Caches are synced for service account
I0524 10:47:10.485320       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0524 10:47:10.487570       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0524 10:47:10.487592       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0524 10:47:10.488725       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0524 10:47:10.488756       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0524 10:47:10.548434       1 shared_informer.go:320] Caches are synced for PVC protection
I0524 10:47:10.553867       1 shared_informer.go:320] Caches are synced for ephemeral
I0524 10:47:10.553902       1 shared_informer.go:320] Caches are synced for persistent volume
I0524 10:47:10.556159       1 shared_informer.go:320] Caches are synced for expand
I0524 10:47:10.578146       1 shared_informer.go:320] Caches are synced for disruption
I0524 10:47:10.581520       1 shared_informer.go:320] Caches are synced for attach detach
I0524 10:47:10.586801       1 shared_informer.go:320] Caches are synced for deployment
I0524 10:47:10.586830       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0524 10:47:10.586905       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="45.427s"
I0524 10:47:10.590074       1 shared_informer.go:320] Caches are synced for stateful set
I0524 10:47:10.673354       1 shared_informer.go:320] Caches are synced for HPA
I0524 10:47:10.676673       1 shared_informer.go:320] Caches are synced for resource quota
I0524 10:47:10.691054       1 shared_informer.go:320] Caches are synced for resource quota
I0524 10:47:11.106409       1 shared_informer.go:320] Caches are synced for garbage collector
I0524 10:47:11.131634       1 shared_informer.go:320] Caches are synced for garbage collector
I0524 10:47:11.131676       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0524 10:56:43.410244       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="34.655187ms"
I0524 10:56:43.418048       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="6.977858ms"
I0524 10:56:43.418156       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="47.472s"
I0524 10:56:43.426679       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="100.334s"
I0524 10:56:43.442909       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="39.467s"
I0524 10:56:48.346583       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="81.658s"
I0524 10:56:59.598013       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="66.197s"
I0524 10:57:15.603714       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="94.171s"
I0524 10:57:30.600375       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="34.196s"
I0524 10:57:47.589421       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="31.45s"
I0524 10:58:02.595975       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="111.464s"
I0524 10:58:45.596244       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="149.182s"
I0524 10:58:58.591372       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mlops-minikube-56fff5959" duration="44.917s"


==> kube-proxy [ae2ce278fe06] <==
I0524 10:46:36.877991       1 server_linux.go:69] "Using iptables proxy"
E0524 10:46:36.879544       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0524 10:46:38.007162       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0524 10:46:40.038728       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0524 10:46:44.738984       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0524 10:46:54.266773       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0524 10:46:54.291467       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0524 10:46:54.291524       1 server_linux.go:165] "Using iptables Proxier"
I0524 10:46:54.292955       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0524 10:46:54.292986       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0524 10:46:54.293008       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0524 10:46:54.293599       1 server.go:872] "Version info" version="v1.30.0"
I0524 10:46:54.293629       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0524 10:46:54.294411       1 config.go:319] "Starting node config controller"
I0524 10:46:54.296212       1 config.go:101] "Starting endpoint slice config controller"
I0524 10:46:54.296265       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0524 10:46:54.294454       1 shared_informer.go:313] Waiting for caches to sync for node config
I0524 10:46:54.294805       1 config.go:192] "Starting service config controller"
I0524 10:46:54.296866       1 shared_informer.go:313] Waiting for caches to sync for service config
I0524 10:46:54.396497       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0524 10:46:54.396559       1 shared_informer.go:320] Caches are synced for node config
I0524 10:46:54.397762       1 shared_informer.go:320] Caches are synced for service config
I0524 10:50:56.864198       1 trace.go:236] Trace[691081189]: "iptables ChainExists" (24-May-2024 10:50:54.351) (total time: 2097ms):
Trace[691081189]: [2.097881075s] [2.097881075s] END
I0524 10:50:56.761322       1 trace.go:236] Trace[1186367741]: "iptables ChainExists" (24-May-2024 10:50:54.352) (total time: 2097ms):
Trace[1186367741]: [2.097464793s] [2.097464793s] END


==> kube-proxy [df597f6fb68a] <==
I0524 10:45:33.580602       1 server_linux.go:69] "Using iptables proxy"
E0524 10:45:33.582455       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0524 10:45:34.741495       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0524 10:45:37.018666       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0524 10:45:41.502907       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E0524 10:45:50.297708       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0524 10:46:07.408325       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0524 10:46:07.433651       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0524 10:46:07.433723       1 server_linux.go:165] "Using iptables Proxier"
I0524 10:46:07.435388       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0524 10:46:07.435429       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0524 10:46:07.435477       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0524 10:46:07.436581       1 server.go:872] "Version info" version="v1.30.0"
I0524 10:46:07.436622       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0524 10:46:07.438173       1 config.go:101] "Starting endpoint slice config controller"
I0524 10:46:07.438217       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0524 10:46:07.438283       1 config.go:319] "Starting node config controller"
I0524 10:46:07.438310       1 shared_informer.go:313] Waiting for caches to sync for node config
I0524 10:46:07.439008       1 config.go:192] "Starting service config controller"
I0524 10:46:07.439037       1 shared_informer.go:313] Waiting for caches to sync for service config
I0524 10:46:07.539081       1 shared_informer.go:320] Caches are synced for node config
I0524 10:46:07.539135       1 shared_informer.go:320] Caches are synced for service config
I0524 10:46:07.539109       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [23ae5a62f921] <==
E0524 10:46:35.169714       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:37.639060       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:37.639104       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:37.842619       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:37.842664       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:38.069642       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:38.069683       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:38.272963       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:38.273008       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:38.706483       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:38.706525       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:39.073074       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:39.073113       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:39.223387       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:39.223432       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:39.436831       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:39.436917       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:39.964561       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:39.964605       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:40.001253       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:40.001300       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:40.005759       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:40.005798       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:40.035457       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:40.035494       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:40.222860       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:40.222905       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:40.470174       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:40.470229       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:40.564874       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:40.564931       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:45.894319       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:45.894358       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:46.487045       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:46.487092       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:46.597945       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:46.597998       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:47.049771       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:47.049836       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:47.224173       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:47.224232       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:47.463483       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:47.463536       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:47.752401       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:47.752448       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:48.197143       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:48.197188       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:48.715702       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:48.715740       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:48.914873       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:48.914936       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:48.947386       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:48.947424       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:49.042240       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:46:49.042295       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:46:51.059454       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0524 10:46:51.059506       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0524 10:46:51.059737       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0524 10:46:51.059755       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
I0524 10:47:05.603086       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [96d177a66222] <==
W0524 10:45:39.128634       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:39.128678       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:41.399021       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:41.399073       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.047265       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.047310       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.048661       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.048688       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.099550       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.099587       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.447606       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.447649       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.499579       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.499616       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.500798       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.500846       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.697030       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.697068       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.727495       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.727538       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:42.851553       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:42.851598       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:43.544553       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:43.544608       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:43.593351       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:43.593423       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:43.700719       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:43.700766       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:44.175125       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:44.175174       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:44.884404       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:44.884454       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:49.059036       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:49.059084       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:50.976086       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:50.976139       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:51.199754       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:51.199810       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:51.249410       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:51.249459       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:51.393583       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:51.393624       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:51.566511       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:51.566559       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:51.629174       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0524 10:45:51.629216       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0524 10:45:53.154388       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0524 10:45:53.154937       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0524 10:45:53.155279       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0524 10:45:53.155577       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0524 10:45:53.156168       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0524 10:45:53.155588       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0524 10:45:53.156197       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0524 10:45:53.156219       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0524 10:45:53.157845       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found]
E0524 10:45:53.157899       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found]
I0524 10:46:05.882707       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0524 10:46:08.958950       1 server.go:214] "waiting for handlers to sync" err="context canceled"
E0524 10:46:08.959233       1 run.go:74] "command failed" err="finished without leader elect"
I0524 10:46:08.959307       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259


==> kubelet <==
May 24 10:46:40 minikube kubelet[1512]: E0524 10:46:40.625768    1512 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?resourceVersion=0&timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 24 10:46:40 minikube kubelet[1512]: E0524 10:46:40.626018    1512 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 24 10:46:40 minikube kubelet[1512]: E0524 10:46:40.626255    1512 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 24 10:46:40 minikube kubelet[1512]: E0524 10:46:40.626496    1512 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 24 10:46:40 minikube kubelet[1512]: E0524 10:46:40.626643    1512 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"minikube\": Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 24 10:46:40 minikube kubelet[1512]: E0524 10:46:40.626652    1512 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
May 24 10:46:43 minikube kubelet[1512]: I0524 10:46:43.601345    1512 scope.go:117] "RemoveContainer" containerID="11bce3018f63e4d8a8335cfd36ba1648321b7f409f3f005bd233ac61db1ac28d"
May 24 10:46:43 minikube kubelet[1512]: E0524 10:46:43.601731    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(7fd44e8d11c3e0ffe6b1825e2a1f2270)\"" pod="kube-system/kube-controller-manager-minikube" podUID="7fd44e8d11c3e0ffe6b1825e2a1f2270"
May 24 10:46:44 minikube kubelet[1512]: E0524 10:46:44.120690    1512 event.go:368] "Unable to write event (may retry after sleeping)" err="Patch \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/events/storage-provisioner.17d266c888031d90\": dial tcp 192.168.49.2:8443: connect: connection refused" event="&Event{ObjectMeta:{storage-provisioner.17d266c888031d90  kube-system   6022 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:kube-system,Name:storage-provisioner,UID:d7b03cd1-3b08-4a0c-8959-936e45c39eff,APIVersion:v1,ResourceVersion:2934,FieldPath:spec.containers{storage-provisioner},},Reason:BackOff,Message:Back-off restarting failed container storage-provisioner in pod storage-provisioner_kube-system(d7b03cd1-3b08-4a0c-8959-936e45c39eff),Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2024-05-24 10:43:39 +0000 UTC,LastTimestamp:2024-05-24 10:46:12.603138256 +0000 UTC m=+182.425287090,Count:6,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
May 24 10:46:45 minikube kubelet[1512]: E0524 10:46:45.833172    1512 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="7s"
May 24 10:46:47 minikube kubelet[1512]: W0524 10:46:47.105846    1512 reflector.go:547] pkg/kubelet/config/apiserver.go:66: failed to list *v1.Pod: Get "https://control-plane.minikube.internal:8443/api/v1/pods?fieldSelector=spec.nodeName%!D(MISSING)minikube&resourceVersion=5944": dial tcp 192.168.49.2:8443: connect: connection refused
May 24 10:46:47 minikube kubelet[1512]: E0524 10:46:47.105908    1512 reflector.go:150] pkg/kubelet/config/apiserver.go:66: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://control-plane.minikube.internal:8443/api/v1/pods?fieldSelector=spec.nodeName%!D(MISSING)minikube&resourceVersion=5944": dial tcp 192.168.49.2:8443: connect: connection refused
May 24 10:46:47 minikube kubelet[1512]: W0524 10:46:47.745368    1512 reflector.go:547] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)coredns&resourceVersion=5945": dial tcp 192.168.49.2:8443: connect: connection refused
May 24 10:46:47 minikube kubelet[1512]: E0524 10:46:47.745448    1512 reflector.go:150] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)coredns&resourceVersion=5945": dial tcp 192.168.49.2:8443: connect: connection refused
May 24 10:46:49 minikube kubelet[1512]: I0524 10:46:49.601160    1512 scope.go:117] "RemoveContainer" containerID="dbf50bd5cbbf98b3eff3d1dc124cff29e0cc01a6b5ed2327d7f3289b7641a463"
May 24 10:46:52 minikube kubelet[1512]: I0524 10:46:52.601518    1512 scope.go:117] "RemoveContainer" containerID="6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c"
May 24 10:46:52 minikube kubelet[1512]: E0524 10:46:52.601758    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d7b03cd1-3b08-4a0c-8959-936e45c39eff)\"" pod="kube-system/storage-provisioner" podUID="d7b03cd1-3b08-4a0c-8959-936e45c39eff"
May 24 10:46:58 minikube kubelet[1512]: I0524 10:46:58.601879    1512 scope.go:117] "RemoveContainer" containerID="11bce3018f63e4d8a8335cfd36ba1648321b7f409f3f005bd233ac61db1ac28d"
May 24 10:47:03 minikube kubelet[1512]: I0524 10:47:03.601160    1512 scope.go:117] "RemoveContainer" containerID="6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c"
May 24 10:47:03 minikube kubelet[1512]: E0524 10:47:03.601434    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d7b03cd1-3b08-4a0c-8959-936e45c39eff)\"" pod="kube-system/storage-provisioner" podUID="d7b03cd1-3b08-4a0c-8959-936e45c39eff"
May 24 10:47:11 minikube kubelet[1512]: I0524 10:47:11.239332    1512 scope.go:117] "RemoveContainer" containerID="796853a56ade0ce0e22b367913dba2c1e9e22ea2198cfc37dd0132f08678b79c"
May 24 10:47:11 minikube kubelet[1512]: I0524 10:47:11.250861    1512 scope.go:117] "RemoveContainer" containerID="42ee514027ee422575613aa715645bbd7d3a73de777957f6cb2b599bae4039f2"
May 24 10:47:18 minikube kubelet[1512]: I0524 10:47:18.600707    1512 scope.go:117] "RemoveContainer" containerID="6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c"
May 24 10:47:18 minikube kubelet[1512]: E0524 10:47:18.600926    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d7b03cd1-3b08-4a0c-8959-936e45c39eff)\"" pod="kube-system/storage-provisioner" podUID="d7b03cd1-3b08-4a0c-8959-936e45c39eff"
May 24 10:47:29 minikube kubelet[1512]: I0524 10:47:29.601321    1512 scope.go:117] "RemoveContainer" containerID="6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c"
May 24 10:47:29 minikube kubelet[1512]: E0524 10:47:29.601522    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d7b03cd1-3b08-4a0c-8959-936e45c39eff)\"" pod="kube-system/storage-provisioner" podUID="d7b03cd1-3b08-4a0c-8959-936e45c39eff"
May 24 10:47:42 minikube kubelet[1512]: I0524 10:47:42.599661    1512 scope.go:117] "RemoveContainer" containerID="6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c"
May 24 10:47:42 minikube kubelet[1512]: E0524 10:47:42.599887    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d7b03cd1-3b08-4a0c-8959-936e45c39eff)\"" pod="kube-system/storage-provisioner" podUID="d7b03cd1-3b08-4a0c-8959-936e45c39eff"
May 24 10:47:53 minikube kubelet[1512]: I0524 10:47:53.599572    1512 scope.go:117] "RemoveContainer" containerID="6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c"
May 24 10:47:53 minikube kubelet[1512]: E0524 10:47:53.599822    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d7b03cd1-3b08-4a0c-8959-936e45c39eff)\"" pod="kube-system/storage-provisioner" podUID="d7b03cd1-3b08-4a0c-8959-936e45c39eff"
May 24 10:48:07 minikube kubelet[1512]: I0524 10:48:07.598790    1512 scope.go:117] "RemoveContainer" containerID="6e9d5eb9fd8bed06ee282e7ee4d420dde6c144e0789add00d153b6ab6bc7de0c"
May 24 10:56:43 minikube kubelet[1512]: I0524 10:56:43.431060    1512 topology_manager.go:215] "Topology Admit Handler" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063" podNamespace="default" podName="mlops-minikube-56fff5959-572ct"
May 24 10:56:43 minikube kubelet[1512]: I0524 10:56:43.563990    1512 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9zhw2\" (UniqueName: \"kubernetes.io/projected/ba791f24-e2f2-42ba-bfdc-92233b578063-kube-api-access-9zhw2\") pod \"mlops-minikube-56fff5959-572ct\" (UID: \"ba791f24-e2f2-42ba-bfdc-92233b578063\") " pod="default/mlops-minikube-56fff5959-572ct"
May 24 10:56:44 minikube kubelet[1512]: I0524 10:56:44.306911    1512 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d7f979e6bbc7721e9456070eef8f94ae444c48f4dd53332edfbb7c8894295fa2"
May 24 10:56:48 minikube kubelet[1512]: E0524 10:56:48.313936    1512 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:56:48 minikube kubelet[1512]: E0524 10:56:48.314365    1512 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:56:48 minikube kubelet[1512]: E0524 10:56:48.316691    1512 kuberuntime_manager.go:1256] container &Container{Name:mlflow-s1-web,Image:mlflow-s1-web:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zhw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mlops-minikube-56fff5959-572ct_default(ba791f24-e2f2-42ba-bfdc-92233b578063): ErrImagePull: Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 24 10:56:48 minikube kubelet[1512]: E0524 10:56:48.318133    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ErrImagePull: \"Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:56:48 minikube kubelet[1512]: E0524 10:56:48.333728    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:57:03 minikube kubelet[1512]: E0524 10:57:03.665192    1512 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:57:03 minikube kubelet[1512]: E0524 10:57:03.665245    1512 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:57:03 minikube kubelet[1512]: E0524 10:57:03.665327    1512 kuberuntime_manager.go:1256] container &Container{Name:mlflow-s1-web,Image:mlflow-s1-web:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zhw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mlops-minikube-56fff5959-572ct_default(ba791f24-e2f2-42ba-bfdc-92233b578063): ErrImagePull: Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 24 10:57:03 minikube kubelet[1512]: E0524 10:57:03.665350    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ErrImagePull: \"Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:57:15 minikube kubelet[1512]: E0524 10:57:15.584679    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:57:34 minikube kubelet[1512]: E0524 10:57:34.208102    1512 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:57:34 minikube kubelet[1512]: E0524 10:57:34.208158    1512 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:57:34 minikube kubelet[1512]: E0524 10:57:34.208247    1512 kuberuntime_manager.go:1256] container &Container{Name:mlflow-s1-web,Image:mlflow-s1-web:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zhw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mlops-minikube-56fff5959-572ct_default(ba791f24-e2f2-42ba-bfdc-92233b578063): ErrImagePull: Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 24 10:57:34 minikube kubelet[1512]: E0524 10:57:34.208270    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ErrImagePull: \"Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:57:47 minikube kubelet[1512]: E0524 10:57:47.582868    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:58:02 minikube kubelet[1512]: E0524 10:58:02.584076    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:58:13 minikube kubelet[1512]: E0524 10:58:13.583292    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:58:32 minikube kubelet[1512]: E0524 10:58:32.206604    1512 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:58:32 minikube kubelet[1512]: E0524 10:58:32.206661    1512 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="mlflow-s1-web:latest"
May 24 10:58:32 minikube kubelet[1512]: E0524 10:58:32.206759    1512 kuberuntime_manager.go:1256] container &Container{Name:mlflow-s1-web,Image:mlflow-s1-web:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zhw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mlops-minikube-56fff5959-572ct_default(ba791f24-e2f2-42ba-bfdc-92233b578063): ErrImagePull: Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 24 10:58:32 minikube kubelet[1512]: E0524 10:58:32.206782    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ErrImagePull: \"Error response from daemon: pull access denied for mlflow-s1-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:58:45 minikube kubelet[1512]: E0524 10:58:45.582434    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:58:58 minikube kubelet[1512]: E0524 10:58:58.582112    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:59:13 minikube kubelet[1512]: E0524 10:59:13.580837    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:59:25 minikube kubelet[1512]: E0524 10:59:25.580537    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"
May 24 10:59:37 minikube kubelet[1512]: E0524 10:59:37.580393    1512 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mlflow-s1-web\" with ImagePullBackOff: \"Back-off pulling image \\\"mlflow-s1-web:latest\\\"\"" pod="default/mlops-minikube-56fff5959-572ct" podUID="ba791f24-e2f2-42ba-bfdc-92233b578063"


==> storage-provisioner [5c0a66e131a8] <==
I0524 10:48:07.722252       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0524 10:48:07.729807       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0524 10:48:07.729900       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0524 10:48:25.165272       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0524 10:48:25.170777       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_02c4b071-fea0-4439-b37e-f7b560c823d2!
I0524 10:48:25.170777       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"29348787-853d-4faa-b63b-d499478a74eb", APIVersion:"v1", ResourceVersion:"6137", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_02c4b071-fea0-4439-b37e-f7b560c823d2 became leader
I0524 10:48:25.379492       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_02c4b071-fea0-4439-b37e-f7b560c823d2!


==> storage-provisioner [6e9d5eb9fd8b] <==
I0524 10:46:36.717944       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0524 10:46:36.720404       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused
